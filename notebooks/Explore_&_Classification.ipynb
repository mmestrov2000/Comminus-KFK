{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a5bb964bb44b4014810899bd0ca97167": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_188f1960cbda4ab891864817b6a15bef",
              "IPY_MODEL_79cafc269d4b42f89136c710d14939d9",
              "IPY_MODEL_8972d6ca20c947179930e17ef41ab151"
            ],
            "layout": "IPY_MODEL_ca7c37c1f2954cc88b54ec369f13bd93"
          }
        },
        "188f1960cbda4ab891864817b6a15bef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c26754e2c6d4cc290a0398c4c0df4a3",
            "placeholder": "​",
            "style": "IPY_MODEL_e2c85912366240c89dfaed1347974081",
            "value": "Loading weights: 100%"
          }
        },
        "79cafc269d4b42f89136c710d14939d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01f67d65f5a848b7a57e06cb7eb9c505",
            "max": 1468,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6bc43b801d274a44b273f44bd1add49b",
            "value": 1468
          }
        },
        "8972d6ca20c947179930e17ef41ab151": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6787b0dc40346afba323ef773ee877e",
            "placeholder": "​",
            "style": "IPY_MODEL_d7fe43c3463f4ec19f58263f878e15bc",
            "value": " 1468/1468 [00:01&lt;00:00, 1365.32it/s, Materializing param=vision_encoder.neck.fpn_layers.3.proj2.weight]"
          }
        },
        "ca7c37c1f2954cc88b54ec369f13bd93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c26754e2c6d4cc290a0398c4c0df4a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2c85912366240c89dfaed1347974081": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01f67d65f5a848b7a57e06cb7eb9c505": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bc43b801d274a44b273f44bd1add49b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6787b0dc40346afba323ef773ee877e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7fe43c3463f4ec19f58263f878e15bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Installs</h1>"
      ],
      "metadata": {
        "id": "aeaTbCyFfU6A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpQ_5NP2e6Ot",
        "outputId": "9616dc14-5e10-415d-f520-a15f5e47b50a",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-bcl1eo0u\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-bcl1eo0u\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 40dc11cd3eb4126652aa41ef8272525affd4a636\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==5.0.0.dev0) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from transformers==5.0.0.dev0) (1.2.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==5.0.0.dev0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==5.0.0.dev0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==5.0.0.dev0) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==5.0.0.dev0) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==5.0.0.dev0) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==5.0.0.dev0) (0.22.1)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers==5.0.0.dev0) (0.20.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==5.0.0.dev0) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==5.0.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0) (1.5.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==5.0.0.dev0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==5.0.0.dev0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==5.0.0.dev0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==5.0.0.dev0) (2025.11.12)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers==5.0.0.dev0) (8.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0) (4.12.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.2.1->transformers==5.0.0.dev0) (0.16.0)\n",
            "Requirement already satisfied: pillow_heif in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: pillow>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from pillow_heif) (11.3.0)\n",
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.12/dist-packages (8.3.237)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cu126)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.18 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.18)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.11.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install pillow_heif\n",
        "!pip install -U ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Google Drive Setup</h1>"
      ],
      "metadata": {
        "id": "QJwo2Ofjfbk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV1_aUSFfZGm",
        "outputId": "4793127b-70c2-40f8-831a-743b52c408bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) PUTANJA DO FOLDERA SA SLIKAMA (prilagodi naziv foldera)\n",
        "IMAGE_DIR = \"/content/drive/MyDrive/Neodata Hackathon 2025/\"\n",
        "\n",
        "# 3) Provjera da svi vide iste slike\n",
        "import os\n",
        "\n",
        "if not os.path.exists(IMAGE_DIR):\n",
        "    raise FileNotFoundError(f\"Folder ne postoji: {IMAGE_DIR}\")\n",
        "\n",
        "print(\"Slike u folderu:\")\n",
        "for f in os.listdir(IMAGE_DIR):\n",
        "    print(\" -\", f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqz6520Hfe1e",
        "outputId": "c662bff8-59b7-4624-ae91-ae18dfb08aa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slike u folderu:\n",
            " - TRAIN\n",
            " - TRAIN_JPG\n",
            " - facade_element1.STP\n",
            " - facade_element2.STP\n",
            " - facade_element3.STP\n",
            " - images\n",
            " - OBJ\n",
            " - YOLO\n",
            " - Neodata2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Data</h1>"
      ],
      "metadata": {
        "id": "RHBLnS1Qfjgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "\n",
        "DATA_ROOT = \"/content/drive/MyDrive/Neodata Hackathon 2025/TRAIN\"\n",
        "\n",
        "TRAIN_DIR = os.path.join(DATA_ROOT, \"\")\n",
        "POS_DIR   = os.path.join(TRAIN_DIR, \"positive\")\n",
        "NEG_DIR   = os.path.join(TRAIN_DIR, \"negative\")\n",
        "\n",
        "for p in [DATA_ROOT, TRAIN_DIR, POS_DIR, NEG_DIR]:\n",
        "    if not os.path.exists(p):\n",
        "        raise FileNotFoundError(f\"Missing: {p}\")\n",
        "\n",
        "print(\"OK. Found:\")\n",
        "print(\" -\", TRAIN_DIR)\n",
        "print(\" - positives:\", len(glob.glob(os.path.join(POS_DIR, \"*\"))))\n",
        "print(\" - negatives:\", len(glob.glob(os.path.join(NEG_DIR, \"*\"))))\n",
        "\n",
        "print(\"\\nExample POS files:\", glob.glob(os.path.join(POS_DIR, \"*\"))[:5])\n",
        "print(\"Example NEG files:\", glob.glob(os.path.join(NEG_DIR, \"*\"))[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcDVY-FYfiLA",
        "outputId": "5979d94e-c226-4803-99e2-41e1e1208016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK. Found:\n",
            " - /content/drive/MyDrive/Neodata Hackathon 2025/TRAIN/\n",
            " - positives: 12\n",
            " - negatives: 43\n",
            "\n",
            "Example POS files: ['/content/drive/MyDrive/Neodata Hackathon 2025/TRAIN/positive/IMG_5675.HEIC', '/content/drive/MyDrive/Neodata Hackathon 2025/TRAIN/positive/IMG_5681.HEIC', '/content/drive/MyDrive/Neodata Hackathon 2025/TRAIN/positive/IMG_5478 3.HEIC', '/content/drive/MyDrive/Neodata Hackathon 2025/TRAIN/positive/IMG_5678.HEIC', '/content/drive/MyDrive/Neodata Hackathon 2025/TRAIN/positive/IMG_5679.HEIC']\n",
            "Example NEG files: ['/content/drive/MyDrive/Neodata Hackathon 2025/TRAIN/negative/IMG_5354 2.HEIC', '/content/drive/MyDrive/Neodata Hackathon 2025/TRAIN/negative/IMG_5464 3.HEIC', '/content/drive/MyDrive/Neodata Hackathon 2025/TRAIN/negative/IMG_5662.HEIC', '/content/drive/MyDrive/Neodata Hackathon 2025/TRAIN/negative/IMG_5401 2.HEIC', '/content/drive/MyDrive/Neodata Hackathon 2025/TRAIN/negative/IMG_5665.HEIC']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3aSwkdqfl3s",
        "outputId": "efd8eed3-25f2-4551-f5ef-9b0c17b81891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import Sam3Processor, Sam3Model\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "model = Sam3Model.from_pretrained(\"facebook/sam3\").to(device)\n",
        "processor = Sam3Processor.from_pretrained(\"facebook/sam3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "a5bb964bb44b4014810899bd0ca97167",
            "188f1960cbda4ab891864817b6a15bef",
            "79cafc269d4b42f89136c710d14939d9",
            "8972d6ca20c947179930e17ef41ab151",
            "ca7c37c1f2954cc88b54ec369f13bd93",
            "0c26754e2c6d4cc290a0398c4c0df4a3",
            "e2c85912366240c89dfaed1347974081",
            "01f67d65f5a848b7a57e06cb7eb9c505",
            "6bc43b801d274a44b273f44bd1add49b",
            "f6787b0dc40346afba323ef773ee877e",
            "d7fe43c3463f4ec19f58263f878e15bc"
          ]
        },
        "id": "SFA_tUilfr4D",
        "outputId": "a97ba98b-03f5-4e1f-cfcf-9b6a95144a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/1468 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5bb964bb44b4014810899bd0ca97167"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Convert to JPG</h1>"
      ],
      "metadata": {
        "id": "NbgImY94f0pZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pillow_heif import register_heif_opener\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "\n",
        "register_heif_opener()\n",
        "\n",
        "OUT_ROOT = \"/content/drive/MyDrive/Neodata Hackathon 2025/TRAIN_JPG\"\n",
        "OUT_POS = f\"{OUT_ROOT}/positive\"\n",
        "OUT_NEG = f\"{OUT_ROOT}/negative\"\n",
        "os.makedirs(OUT_POS, exist_ok=True)\n",
        "os.makedirs(OUT_NEG, exist_ok=True)"
      ],
      "metadata": {
        "id": "WGRA0kIRfxZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "POS_JPG = sorted(glob.glob(os.path.join(OUT_POS, \"*.jpg\")))\n",
        "NEG_JPG = sorted(glob.glob(os.path.join(OUT_NEG, \"*.jpg\")))\n",
        "\n",
        "img_path = random.choice(NEG_JPG)   # switch to NEG_JPG if you want\n",
        "print(\"Using:\", img_path)\n",
        "\n",
        "image = Image.open(img_path).convert(\"RGB\")\n",
        "image"
      ],
      "metadata": {
        "id": "jR0VnpbEf4oX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "prompt = \"large orange wavy area\"  # try: \"hole\", \"glass\", \"rubber gasket\", \"metal sheet\"\n",
        "\n",
        "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "results = processor.post_process_instance_segmentation(\n",
        "    outputs,\n",
        "    threshold=0.6,\n",
        "    mask_threshold=0.5,\n",
        "    target_sizes=inputs.get(\"original_sizes\").tolist()\n",
        ")[0]\n",
        "\n",
        "print(f\"Prompt: {prompt} | Found {len(results['masks'])} instances\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL52Nw6Of7mn",
        "outputId": "433ec732-cc7a-4294-822b-23e6c5ef1628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: large orange wavy area | Found 0 instances\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "from PIL import Image as PILImage\n",
        "\n",
        "def overlay_masks(image, masks):\n",
        "    image = image.convert(\"RGBA\")\n",
        "    masks = 255 * masks.cpu().numpy().astype(np.uint8)\n",
        "\n",
        "    n_masks = masks.shape[0]\n",
        "    if n_masks == 0:\n",
        "        return image\n",
        "\n",
        "    cmap = matplotlib.colormaps.get_cmap(\"rainbow\").resampled(n_masks)\n",
        "    colors = [tuple(int(c * 255) for c in cmap(i)[:3]) for i in range(n_masks)]\n",
        "\n",
        "    for mask, color in zip(masks, colors):\n",
        "        mask_img = PILImage.fromarray(mask)\n",
        "        overlay = PILImage.new(\"RGBA\", image.size, color + (0,))\n",
        "        alpha = mask_img.point(lambda v: int(v * 0.5))\n",
        "        overlay.putalpha(alpha)\n",
        "        image = PILImage.alpha_composite(image, overlay)\n",
        "    return image\n",
        "\n",
        "viz = overlay_masks(image, results[\"masks\"])\n",
        "viz"
      ],
      "metadata": {
        "id": "9qrIoMQOgFaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Classification (Facade element)</h1>"
      ],
      "metadata": {
        "id": "go1W_tuBgL-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train YOLO"
      ],
      "metadata": {
        "id": "43HyhodogO8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import cv2\n",
        "from collections import Counter\n",
        "\n",
        "# ====== PATHS ======\n",
        "YOLO_ROOT = Path(\"/content/drive/MyDrive/Neodata Hackathon 2025/YOLO\")\n",
        "DATASET = YOLO_ROOT / \"dataset\"\n",
        "\n",
        "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"}\n",
        "\n",
        "# Desired copies (you can keep fixed OR auto-tune below)\n",
        "# If you want fixed, set these and set AUTO_TUNE=False\n",
        "N_FOR_CLASS0 = 7\n",
        "N_FOR_CLASS2 = 43\n",
        "N_FOR_EMPTY  = 43\n",
        "\n",
        "AUTO_TUNE = True  # auto-adjust based on TRAIN distribution you gave (recommended)\n",
        "\n",
        "SPLITS = [\"train\", \"val\"]  # <-- now includes val\n",
        "\n",
        "def find_image(img_dir: Path, stem: str) -> Path | None:\n",
        "    for p in img_dir.glob(stem + \".*\"):\n",
        "        if p.suffix.lower() in IMG_EXTS:\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "def parse_classes(txt_path: Path) -> list[int]:\n",
        "    classes = []\n",
        "    for ln in txt_path.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines():\n",
        "        ln = ln.strip()\n",
        "        if not ln:\n",
        "            continue\n",
        "        parts = ln.split()\n",
        "        if not parts:\n",
        "            continue\n",
        "        try:\n",
        "            classes.append(int(float(parts[0])))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return classes\n",
        "\n",
        "def auto_tune_counts_on_train():\n",
        "    \"\"\"Tune N_FOR_CLASS0/N_FOR_CLASS2/N_FOR_EMPTY from TRAIN distribution only.\"\"\"\n",
        "    train_lbl_dir = DATASET / \"labels\" / \"train\"\n",
        "    label_files = sorted(train_lbl_dir.glob(\"*.txt\"))\n",
        "\n",
        "    file_counts = Counter()   # counts of files containing class c\n",
        "    empty_files = 0\n",
        "\n",
        "    for lp in label_files:\n",
        "        cls = set(parse_classes(lp))\n",
        "        if not cls:\n",
        "            empty_files += 1\n",
        "            continue\n",
        "        for c in cls:\n",
        "            file_counts[c] += 1\n",
        "\n",
        "    c0, c1, c2 = file_counts.get(0, 0), file_counts.get(1, 0), file_counts.get(2, 0)\n",
        "    target = max(c1, 1)  # bring others roughly up to class1 count\n",
        "\n",
        "    import math\n",
        "    def per_file_copies(current):\n",
        "        if current <= 0:\n",
        "            return 0\n",
        "        need = max(0, target - current)\n",
        "        return max(0, math.ceil(need / current))\n",
        "\n",
        "    n0 = min(per_file_copies(c0), 50)\n",
        "    n2 = min(per_file_copies(c2), 200)\n",
        "    ne = min(per_file_copies(empty_files), 100)\n",
        "\n",
        "    print(\"\\n=== TRAIN distribution (for auto-tune) ===\")\n",
        "    print(\"class0 files:\", c0, \"class1 files:\", c1, \"class2 files:\", c2, \"empty files:\", empty_files)\n",
        "    print(\"Auto target ~\", target)\n",
        "    print(\"Auto-chosen: N_FOR_CLASS0 =\", n0, \"N_FOR_CLASS2 =\", n2, \"N_FOR_EMPTY =\", ne)\n",
        "    return n0, n2, ne\n",
        "\n",
        "if AUTO_TUNE:\n",
        "    N_FOR_CLASS0, N_FOR_CLASS2, N_FOR_EMPTY = auto_tune_counts_on_train()\n",
        "\n",
        "total_created = 0\n",
        "\n",
        "for split in SPLITS:\n",
        "    img_dir = DATASET / \"images\" / split\n",
        "    lbl_dir = DATASET / \"labels\" / split\n",
        "\n",
        "    print(f\"\\n=== Split: {split} ===\")\n",
        "    print(\"Images dir:\", img_dir)\n",
        "    print(\"Labels dir:\", lbl_dir)\n",
        "\n",
        "    assert img_dir.exists(), f\"Missing images dir: {img_dir}\"\n",
        "    assert lbl_dir.exists(), f\"Missing labels dir: {lbl_dir}\"\n",
        "\n",
        "    label_files = sorted(lbl_dir.glob(\"*.txt\"))\n",
        "    print(\"Found label files:\", len(label_files))\n",
        "\n",
        "    created = 0\n",
        "    skipped_no_image = 0\n",
        "    skipped_existing = 0\n",
        "    skipped_other = 0\n",
        "\n",
        "    for lbl_path in label_files:\n",
        "        stem = lbl_path.stem\n",
        "        classes = parse_classes(lbl_path)\n",
        "        cls_set = set(classes)\n",
        "\n",
        "        # Decide how many copies to create\n",
        "        if len(classes) == 0:\n",
        "            n_copies = N_FOR_EMPTY\n",
        "        elif 2 in cls_set:\n",
        "            n_copies = N_FOR_CLASS2\n",
        "        elif 0 in cls_set:\n",
        "            n_copies = N_FOR_CLASS0\n",
        "        else:\n",
        "            skipped_other += 1\n",
        "            continue\n",
        "\n",
        "        if n_copies <= 0:\n",
        "            continue\n",
        "\n",
        "        img_path = find_image(img_dir, stem)\n",
        "        if img_path is None:\n",
        "            skipped_no_image += 1\n",
        "            continue\n",
        "\n",
        "        img = cv2.imread(str(img_path))\n",
        "        if img is None:\n",
        "            skipped_no_image += 1\n",
        "            continue\n",
        "\n",
        "        # ONLY 90° rotation (computed once)\n",
        "        img90 = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
        "\n",
        "        for i in range(1, n_copies + 1):\n",
        "            new_stem = f\"{stem}_rotation{i}\"\n",
        "            new_img_path = img_path.with_name(new_stem + img_path.suffix)\n",
        "            new_lbl_path = lbl_path.with_name(new_stem + \".txt\")\n",
        "\n",
        "            # Don’t overwrite\n",
        "            if new_img_path.exists() or new_lbl_path.exists():\n",
        "                skipped_existing += 1\n",
        "                continue\n",
        "\n",
        "            ok = cv2.imwrite(str(new_img_path), img90)\n",
        "            if not ok:\n",
        "                raise RuntimeError(f\"Failed to write image: {new_img_path}\")\n",
        "\n",
        "            # Labels: full-image boxes; empty stays empty\n",
        "            if len(classes) == 0:\n",
        "                new_lbl_path.write_text(\"\", encoding=\"utf-8\")\n",
        "            else:\n",
        "                lines = [f\"{c} 0.5 0.5 1.0 1.0\" for c in classes]\n",
        "                new_lbl_path.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n",
        "\n",
        "            created += 1\n",
        "\n",
        "    total_created += created\n",
        "    print(\"Created:\", created)\n",
        "    print(\"Skipped (no matching image):\", skipped_no_image)\n",
        "    print(\"Skipped (already existed):\", skipped_existing)\n",
        "    print(\"Skipped (not class0/class2 and not empty):\", skipped_other)\n",
        "\n",
        "print(f\"\\nAll done. Total created across train+val: {total_created}\")"
      ],
      "metadata": {
        "id": "UwYxVyNDgLYy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "700efe3a-8b28-476b-f332-eb235e0b87c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TRAIN distribution (for auto-tune) ===\n",
            "class0 files: 5 class1 files: 40 class2 files: 1 empty files: 1\n",
            "Auto target ~ 40\n",
            "Auto-chosen: N_FOR_CLASS0 = 7 N_FOR_CLASS2 = 39 N_FOR_EMPTY = 39\n",
            "\n",
            "=== Split: train ===\n",
            "Images dir: /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/train\n",
            "Labels dir: /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/labels/train\n",
            "Found label files: 47\n",
            "Created: 113\n",
            "Skipped (no matching image): 0\n",
            "Skipped (already existed): 0\n",
            "Skipped (not class0/class2 and not empty): 40\n",
            "\n",
            "=== Split: val ===\n",
            "Images dir: /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val\n",
            "Labels dir: /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/labels/val\n",
            "Found label files: 9\n",
            "Created: 92\n",
            "Skipped (no matching image): 0\n",
            "Skipped (already existed): 0\n",
            "Skipped (not class0/class2 and not empty): 5\n",
            "\n",
            "All done. Total created across train+val: 205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_YAML = DATASET / \"facade.yaml\"\n",
        "PROJECT = YOLO_ROOT / \"runs\"\n",
        "NAME = \"yolov8n_balanced_train\"\n",
        "\n",
        "DEVICE = 0 if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "MODEL_WEIGHTS = \"yolov8n.pt\"\n",
        "model = YOLO(MODEL_WEIGHTS)\n",
        "\n",
        "# --- Sanity check: ensure val labels are single-class per image ---\n",
        "VAL_LABELS = DATASET / \"labels\" / \"val\"\n",
        "multi_class_files = []\n",
        "for p in sorted(VAL_LABELS.glob(\"*.txt\")):\n",
        "    lines = [ln.strip() for ln in p.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines() if ln.strip()]\n",
        "    if not lines:\n",
        "        continue\n",
        "    classes = []\n",
        "    for ln in lines:\n",
        "        try:\n",
        "            classes.append(int(float(ln.split()[0])))\n",
        "        except Exception:\n",
        "            pass\n",
        "    if len(set(classes)) > 1:\n",
        "        multi_class_files.append((p.name, sorted(set(classes))))\n",
        "\n",
        "if multi_class_files:\n",
        "    print(\"\\n⚠️ WARNING: Some VAL label files contain MULTIPLE classes (this breaks your assumption).\")\n",
        "    print(\"First few examples:\")\n",
        "    for name, cls in multi_class_files[:10]:\n",
        "        print(\" -\", name, \"classes:\", cls)\n",
        "else:\n",
        "    print(\"\\n✅ VAL labels look single-class per image (or empty).\")\n",
        "\n",
        "# Train\n",
        "train_results = model.train(\n",
        "    data=str(DATA_YAML),\n",
        "    epochs=30,\n",
        "    patience=8,\n",
        "    imgsz=384,\n",
        "    batch=16 if DEVICE != \"cpu\" else 4,\n",
        "    workers=2,\n",
        "    device=DEVICE,\n",
        "    amp=(DEVICE != \"cpu\"),\n",
        "    cache=True,\n",
        "    lr0=0.003,\n",
        "    warmup_epochs=2,\n",
        "    close_mosaic=10,\n",
        "    project=str(PROJECT),\n",
        "    name=NAME,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# Validate strictly on val split\n",
        "metrics = model.val(data=str(DATA_YAML), device=DEVICE, split=\"val\")\n",
        "box = metrics.box\n",
        "\n",
        "precision = float(box.mp)\n",
        "recall    = float(box.mr)\n",
        "f1        = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0\n",
        "map50     = float(box.map50)\n",
        "map5095   = float(box.map)\n",
        "\n",
        "print(\"\\n=== VAL metrics (detection) ===\")\n",
        "print(f\"Precision (mean): {precision:.4f}\")\n",
        "print(f\"Recall    (mean): {recall:.4f}\")\n",
        "print(f\"F1        (mean): {f1:.4f}\")\n",
        "print(f\"mAP@0.5         : {map50:.4f}\")\n",
        "print(f\"mAP@0.5:0.95    : {map5095:.4f}\")\n",
        "\n",
        "print(\"\\nBest model weights saved at:\")\n",
        "print(PROJECT / NAME / \"weights\" / \"best.pt\")"
      ],
      "metadata": {
        "id": "unz_5MGJgYe8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14596b58-c667-43e5-f4b7-2b42c9fafb63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: 0\n",
            "\n",
            "✅ VAL labels look single-class per image (or empty).\n",
            "Ultralytics 8.3.237 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=True, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/facade.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=384, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.003, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8n_balanced_train5, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=8, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/Neodata Hackathon 2025/YOLO/runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/.shortcut-targets-by-id/1eACqbAyiu4G3bgvb1aMnfl0pjOD46Nka/Neodata Hackathon 2025/YOLO/runs/yolov8n_balanced_train5, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=2, warmup_momentum=0.8, weight_decay=0.0005, workers=2, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=3\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751897  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,011,433 parameters, 3,011,417 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ultralytics/nn/modules/block.py:1299: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:315.)\n",
            "  attn = (q.transpose(-2, -1) @ k) * self.scale\n",
            "/usr/local/lib/python3.12/dist-packages/ultralytics/nn/modules/block.py:1301: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:315.)\n",
            "  x = (v @ attn.transpose(-2, -1)).view(B, C, H, W) + self.pe(v.reshape(B, C, H, W))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.6±0.1 ms, read: 644.0±353.5 MB/s, size: 5153.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/.shortcut-targets-by-id/1eACqbAyiu4G3bgvb1aMnfl0pjOD46Nka/Neodata Hackathon 2025/YOLO/dataset/labels/train.cache... 160 images, 40 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 160/160 227.9Kit/s 0.0s\n",
            "WARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB RAM): 100% ━━━━━━━━━━━━ 160/160 36.8it/s 4.4s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.9±0.9 ms, read: 591.6±336.6 MB/s, size: 5830.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/.shortcut-targets-by-id/1eACqbAyiu4G3bgvb1aMnfl0pjOD46Nka/Neodata Hackathon 2025/YOLO/dataset/labels/val.cache... 101 images, 40 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 101/101 131.8Kit/s 0.0s\n",
            "WARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB RAM): 100% ━━━━━━━━━━━━ 101/101 30.9it/s 3.3s\n",
            "Plotting labels to /content/drive/.shortcut-targets-by-id/1eACqbAyiu4G3bgvb1aMnfl0pjOD46Nka/Neodata Hackathon 2025/YOLO/runs/yolov8n_balanced_train5/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.003' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 384 train, 384 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/.shortcut-targets-by-id/1eACqbAyiu4G3bgvb1aMnfl0pjOD46Nka/Neodata Hackathon 2025/YOLO/runs/yolov8n_balanced_train5\u001b[0m\n",
            "Starting training for 30 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/30         1G     0.8301      2.934      1.258         36        384: 0% ──────────── 0/10  0.1s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ultralytics/utils/loss.py:238: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:315.)\n",
            "  pred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K       1/30      1.04G     0.8263      2.895      1.287         31        384: 100% ━━━━━━━━━━━━ 10/10 9.6it/s 1.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 11.7it/s 0.3s\n",
            "                   all        101         61    0.00201          1      0.451      0.409\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/30      1.08G     0.3767      2.607      1.044         39        384: 10% ━─────────── 1/10 1.7it/s 0.2s<5.2s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ultralytics/utils/loss.py:238: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:315.)\n",
            "  pred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K       2/30      1.08G     0.3029      2.283     0.9831         39        384: 100% ━━━━━━━━━━━━ 10/10 12.0it/s 0.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 11.6it/s 0.3s\n",
            "                   all        101         61     0.0022          1      0.732       0.69\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/30      1.14G      0.225      1.825     0.9188         28        384: 0% ──────────── 0/10  0.1s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ultralytics/utils/loss.py:238: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:315.)\n",
            "  pred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K       3/30      1.14G     0.2573      1.614     0.9428         44        384: 100% ━━━━━━━━━━━━ 10/10 12.0it/s 0.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 11.4it/s 0.4s\n",
            "                   all        101         61      0.761      0.458      0.503      0.385\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/30      1.14G     0.2153      1.205     0.9202         31        384: 10% ━─────────── 1/10 1.9it/s 0.2s<4.6s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ultralytics/utils/loss.py:238: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:315.)\n",
            "  pred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K       4/30      1.14G     0.2163      1.129     0.9143         33        384: 100% ━━━━━━━━━━━━ 10/10 13.0it/s 0.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 12.6it/s 0.3s\n",
            "                   all        101         61      0.881      0.458      0.627      0.538\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/30      1.14G     0.2279      1.006     0.8749         43        384: 10% ━─────────── 1/10 1.9it/s 0.2s<4.6s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ultralytics/utils/loss.py:238: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:315.)\n",
            "  pred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K       5/30      1.14G     0.2403     0.9504     0.9053         35        384: 100% ━━━━━━━━━━━━ 10/10 12.4it/s 0.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 12.4it/s 0.3s\n",
            "                   all        101         61       0.99      0.533      0.723      0.649\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/30      1.14G     0.2655     0.9467      0.893         35        384: 0% ──────────── 0/10  0.1s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ultralytics/utils/loss.py:238: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:315.)\n",
            "  pred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K       6/30      1.14G     0.2721     0.8557      0.903         44        384: 100% ━━━━━━━━━━━━ 10/10 13.0it/s 0.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 12.6it/s 0.3s\n",
            "                   all        101         61      0.756      0.812      0.781      0.721\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/30      1.16G     0.2365     0.7291     0.9006         32        384: 10% ━─────────── 1/10 2.3it/s 0.1s<4.0s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ultralytics/utils/loss.py:238: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:315.)\n",
            "  pred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K       7/30      1.16G     0.2736     0.7933     0.9186         37        384: 100% ━━━━━━━━━━━━ 10/10 13.9it/s 0.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 12.5it/s 0.3s\n",
            "                   all        101         61      0.899      0.928      0.995      0.961\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/30      1.16G     0.2569     0.8347     0.9249         42        384: 10% ━─────────── 1/10 1.9it/s 0.2s<4.9s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ultralytics/utils/loss.py:238: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:315.)\n",
            "  pred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K       8/30      1.16G      0.243      0.753     0.9084         39        384: 100% ━━━━━━━━━━━━ 10/10 13.5it/s 0.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 12.5it/s 0.3s\n",
            "                   all        101         61      0.872      0.832      0.983      0.939\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/30      1.17G      0.242     0.6444     0.8931         37        384: 10% ━─────────── 1/10 2.0it/s 0.2s<4.5s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ultralytics/utils/loss.py:238: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:315.)\n",
            "  pred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K       9/30      1.17G     0.2618     0.7043      0.918         33        384: 100% ━━━━━━━━━━━━ 10/10 13.7it/s 0.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 12.1it/s 0.3s\n",
            "                   all        101         61      0.786      0.959      0.878       0.87\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/30      1.17G     0.2476     0.6322     0.8732         38        384: 10% ━─────────── 1/10 1.9it/s 0.2s<4.8s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ultralytics/utils/loss.py:238: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:315.)\n",
            "  pred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      10/30      1.17G     0.2306     0.6653      0.892         35        384: 100% ━━━━━━━━━━━━ 10/10 13.3it/s 0.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 12.1it/s 0.3s\n",
            "                   all        101         61      0.598      0.781      0.855      0.804\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/30      1.19G     0.2502     0.7757     0.9008         38        384: 10% ━─────────── 1/10 2.3it/s 0.1s<4.0s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ultralytics/utils/loss.py:238: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:315.)\n",
            "  pred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      11/30      1.19G     0.2449      0.653     0.9133         42        384: 100% ━━━━━━━━━━━━ 10/10 14.2it/s 0.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 12.1it/s 0.3s\n",
            "                   all        101         61      0.633      0.771       0.82      0.769\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/30      1.19G      0.265     0.6706     0.9182         40        384: 10% ━─────────── 1/10 2.0it/s 0.2s<4.6s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ultralytics/utils/loss.py:238: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:315.)\n",
            "  pred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      12/30      1.19G     0.2414     0.6133     0.9097         33        384: 100% ━━━━━━━━━━━━ 10/10 13.6it/s 0.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 12.8it/s 0.3s\n",
            "                   all        101         61      0.527      0.713      0.805      0.676\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/30      1.19G     0.2117     0.6916     0.9205         38        384: 10% ━─────────── 1/10 2.2it/s 0.1s<4.0s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ultralytics/utils/loss.py:238: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:315.)\n",
            "  pred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      13/30      1.19G     0.2126     0.6377      0.921         39        384: 100% ━━━━━━━━━━━━ 10/10 13.9it/s 0.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 12.6it/s 0.3s\n",
            "                   all        101         61      0.695      0.454      0.574      0.569\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/30      1.19G     0.1968     0.5719     0.9143         31        384: 10% ━─────────── 1/10 1.9it/s 0.2s<4.6s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ultralytics/utils/loss.py:238: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:315.)\n",
            "  pred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      14/30      1.19G     0.2095     0.6367     0.9051         36        384: 100% ━━━━━━━━━━━━ 10/10 13.8it/s 0.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 12.9it/s 0.3s\n",
            "                   all        101         61       0.65      0.857      0.859      0.855\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/30      1.19G     0.2115     0.5314     0.9042         33        384: 10% ━─────────── 1/10 2.3it/s 0.1s<3.9s"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ultralytics/utils/loss.py:238: UserWarning: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:315.)\n",
            "  pred_dist = pred_dist.view(b, a, 4, c // 4).softmax(3).matmul(self.proj.type(pred_dist.dtype))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K      15/30      1.19G     0.2195     0.5939     0.9128         35        384: 100% ━━━━━━━━━━━━ 10/10 14.1it/s 0.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 13.0it/s 0.3s\n",
            "                   all        101         61      0.888        0.8      0.781      0.755\n",
            "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 8 epochs. Best results observed at epoch 7, best model saved as best.pt.\n",
            "To update EarlyStopping(patience=8) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
            "\n",
            "15 epochs completed in 0.006 hours.\n",
            "Optimizer stripped from /content/drive/.shortcut-targets-by-id/1eACqbAyiu4G3bgvb1aMnfl0pjOD46Nka/Neodata Hackathon 2025/YOLO/runs/yolov8n_balanced_train5/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/drive/.shortcut-targets-by-id/1eACqbAyiu4G3bgvb1aMnfl0pjOD46Nka/Neodata Hackathon 2025/YOLO/runs/yolov8n_balanced_train5/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/drive/.shortcut-targets-by-id/1eACqbAyiu4G3bgvb1aMnfl0pjOD46Nka/Neodata Hackathon 2025/YOLO/runs/yolov8n_balanced_train5/weights/best.pt...\n",
            "Ultralytics 8.3.237 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "Model summary (fused): 72 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 6.2it/s 0.6s\n",
            "                   all        101         61        0.9      0.927      0.995      0.961\n",
            "       facade_element0         16         16      0.862          1      0.995      0.977\n",
            "       facade_element1          5          5          1      0.781      0.995       0.91\n",
            "       facade_element2         40         40      0.838          1      0.995      0.995\n",
            "Speed: 0.1ms preprocess, 0.7ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/.shortcut-targets-by-id/1eACqbAyiu4G3bgvb1aMnfl0pjOD46Nka/Neodata Hackathon 2025/YOLO/runs/yolov8n_balanced_train5\u001b[0m\n",
            "Ultralytics 8.3.237 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "Model summary (fused): 72 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.5±0.1 ms, read: 758.2±75.1 MB/s, size: 6195.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/.shortcut-targets-by-id/1eACqbAyiu4G3bgvb1aMnfl0pjOD46Nka/Neodata Hackathon 2025/YOLO/dataset/labels/val.cache... 101 images, 40 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 101/101 192.1Kit/s 0.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 7/7 1.4it/s 5.1s\n",
            "                   all        101         61      0.899      0.927      0.995      0.961\n",
            "       facade_element0         16         16       0.86          1      0.995      0.977\n",
            "       facade_element1          5          5          1      0.782      0.995       0.91\n",
            "       facade_element2         40         40      0.838          1      0.995      0.995\n",
            "Speed: 0.4ms preprocess, 0.9ms inference, 0.0ms loss, 2.7ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/val4\u001b[0m\n",
            "\n",
            "=== VAL metrics (detection) ===\n",
            "Precision (mean): 0.8994\n",
            "Recall    (mean): 0.9275\n",
            "F1        (mean): 0.9132\n",
            "mAP@0.5         : 0.9950\n",
            "mAP@0.5:0.95    : 0.9607\n",
            "\n",
            "Best model weights saved at:\n",
            "/content/drive/MyDrive/Neodata Hackathon 2025/YOLO/runs/yolov8n_balanced_train/weights/best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "VAL_IMAGES = DATASET / \"images\" / \"val\"\n",
        "VAL_LABELS = DATASET / \"labels\" / \"val\"\n",
        "\n",
        "assert VAL_IMAGES.exists(), f\"Missing: {VAL_IMAGES}\"\n",
        "assert VAL_LABELS.exists(), f\"Missing: {VAL_LABELS}\"\n",
        "\n",
        "# Predict (in-memory)\n",
        "results = model.predict(\n",
        "    source=str(VAL_IMAGES),\n",
        "    imgsz=384,\n",
        "    conf=0.25,\n",
        "    iou=0.5,\n",
        "    save=True,\n",
        "    save_txt=True,\n",
        "    project=str(PROJECT),\n",
        "    name=f\"{NAME}_pred_val\",\n",
        "    device=DEVICE,\n",
        ")\n",
        "\n",
        "rows = []\n",
        "\n",
        "def read_single_gt_class(label_path: Path):\n",
        "    \"\"\"Return (gt_class or None, flag_multiclass:bool).\"\"\"\n",
        "    if not label_path.exists():\n",
        "        return None, False\n",
        "    classes = []\n",
        "    for ln in label_path.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines():\n",
        "        ln = ln.strip()\n",
        "        if not ln:\n",
        "            continue\n",
        "        try:\n",
        "            classes.append(int(float(ln.split()[0])))\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not classes:\n",
        "        return None, False\n",
        "    uniq = sorted(set(classes))\n",
        "    return uniq[0], (len(uniq) > 1)\n",
        "\n",
        "for res in results:\n",
        "    img_path = Path(res.path)\n",
        "    stem = img_path.stem\n",
        "\n",
        "    # ---- SINGLE predicted class per image ----\n",
        "    pred_class = None\n",
        "    pred_conf = None\n",
        "\n",
        "    if res.boxes is not None and len(res.boxes) > 0:\n",
        "        confs = res.boxes.conf.detach().cpu().numpy()\n",
        "        clss  = res.boxes.cls.detach().cpu().numpy().astype(int)\n",
        "        best_i = int(np.argmax(confs))\n",
        "        pred_class = int(clss[best_i])\n",
        "        pred_conf = float(confs[best_i])\n",
        "\n",
        "    # ---- SINGLE real class per image ----\n",
        "    label_path = VAL_LABELS / f\"{stem}.txt\"\n",
        "    real_class, gt_multiclass = read_single_gt_class(label_path)\n",
        "\n",
        "    rows.append({\n",
        "        \"image\": img_path.name,\n",
        "        \"predict\": pred_class,          # ONE class or None\n",
        "        \"pred_conf\": pred_conf,         # confidence of chosen class\n",
        "        \"real\": real_class,             # ONE class or None\n",
        "        \"gt_multiclass\": gt_multiclass  # True if your GT violates assumption\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(rows).sort_values(\"image\")\n",
        "\n",
        "print(\"\\n=== ONE-class Prediction vs Ground Truth (VAL) ===\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "csv_path = PROJECT / NAME / \"val_prediction_vs_gt_oneclass.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "print(f\"\\nSaved CSV to: {csv_path}\")\n",
        "\n",
        "print(f\"Annotated predictions saved to: {PROJECT / (NAME + '_pred_val')}\")"
      ],
      "metadata": {
        "id": "Q1PLdjpIgaFt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f048f0fb-5c40-4d7a-c3ea-96a04fc85f05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5346 2.jpg: 384x288 1 facade_element1, 8.8ms\n",
            "image 2/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5349 2.jpg: 384x288 1 facade_element1, 7.7ms\n",
            "image 3/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5350 2.jpg: 288x384 1 facade_element0, 1 facade_element1, 9.6ms\n",
            "image 4/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5354 2.jpg: 288x384 1 facade_element0, 1 facade_element1, 7.5ms\n",
            "image 5/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5443 3.jpg: 288x384 1 facade_element0, 1 facade_element1, 7.5ms\n",
            "image 6/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5620.jpg: 288x384 1 facade_element0, 7.7ms\n",
            "image 7/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5620_rotation1.jpg: 384x288 1 facade_element0, 2 facade_element2s, 8.2ms\n",
            "image 8/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5620_rotation2.jpg: 384x288 1 facade_element0, 2 facade_element2s, 7.9ms\n",
            "image 9/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5620_rotation3.jpg: 384x288 1 facade_element0, 2 facade_element2s, 7.6ms\n",
            "image 10/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5620_rotation4.jpg: 384x288 1 facade_element0, 2 facade_element2s, 7.4ms\n",
            "image 11/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5620_rotation5.jpg: 384x288 1 facade_element0, 2 facade_element2s, 7.7ms\n",
            "image 12/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5620_rotation6.jpg: 384x288 1 facade_element0, 2 facade_element2s, 7.6ms\n",
            "image 13/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5620_rotation7.jpg: 384x288 1 facade_element0, 2 facade_element2s, 7.5ms\n",
            "image 14/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623.jpg: 288x384 1 facade_element0, 2 facade_element1s, 8.2ms\n",
            "image 15/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation1.jpg: 384x288 (no detections), 8.1ms\n",
            "image 16/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation10.jpg: 384x288 (no detections), 7.4ms\n",
            "image 17/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation11.jpg: 384x288 (no detections), 7.6ms\n",
            "image 18/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation12.jpg: 384x288 (no detections), 7.6ms\n",
            "image 19/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation13.jpg: 384x288 (no detections), 7.6ms\n",
            "image 20/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation14.jpg: 384x288 (no detections), 7.7ms\n",
            "image 21/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation15.jpg: 384x288 (no detections), 7.6ms\n",
            "image 22/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation16.jpg: 384x288 (no detections), 8.1ms\n",
            "image 23/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation17.jpg: 384x288 (no detections), 7.5ms\n",
            "image 24/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation18.jpg: 384x288 (no detections), 8.1ms\n",
            "image 25/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation19.jpg: 384x288 (no detections), 7.9ms\n",
            "image 26/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation2.jpg: 384x288 (no detections), 7.5ms\n",
            "image 27/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation20.jpg: 384x288 (no detections), 7.4ms\n",
            "image 28/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation21.jpg: 384x288 (no detections), 7.6ms\n",
            "image 29/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation22.jpg: 384x288 (no detections), 7.5ms\n",
            "image 30/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation23.jpg: 384x288 (no detections), 7.8ms\n",
            "image 31/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation24.jpg: 384x288 (no detections), 7.6ms\n",
            "image 32/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation25.jpg: 384x288 (no detections), 7.5ms\n",
            "image 33/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation26.jpg: 384x288 (no detections), 7.5ms\n",
            "image 34/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation27.jpg: 384x288 (no detections), 7.5ms\n",
            "image 35/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation28.jpg: 384x288 (no detections), 7.6ms\n",
            "image 36/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation29.jpg: 384x288 (no detections), 7.4ms\n",
            "image 37/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation3.jpg: 384x288 (no detections), 7.6ms\n",
            "image 38/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation30.jpg: 384x288 (no detections), 7.6ms\n",
            "image 39/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation31.jpg: 384x288 (no detections), 7.5ms\n",
            "image 40/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation32.jpg: 384x288 (no detections), 7.5ms\n",
            "image 41/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation33.jpg: 384x288 (no detections), 7.6ms\n",
            "image 42/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation34.jpg: 384x288 (no detections), 7.6ms\n",
            "image 43/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation35.jpg: 384x288 (no detections), 8.7ms\n",
            "image 44/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation36.jpg: 384x288 (no detections), 7.9ms\n",
            "image 45/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation37.jpg: 384x288 (no detections), 7.5ms\n",
            "image 46/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation38.jpg: 384x288 (no detections), 7.5ms\n",
            "image 47/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation39.jpg: 384x288 (no detections), 8.8ms\n",
            "image 48/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation4.jpg: 384x288 (no detections), 7.6ms\n",
            "image 49/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation5.jpg: 384x288 (no detections), 7.6ms\n",
            "image 50/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation6.jpg: 384x288 (no detections), 7.6ms\n",
            "image 51/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation7.jpg: 384x288 (no detections), 7.6ms\n",
            "image 52/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation8.jpg: 384x288 (no detections), 7.7ms\n",
            "image 53/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5623_rotation9.jpg: 384x288 (no detections), 7.7ms\n",
            "image 54/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5640.jpg: 288x384 2 facade_element0s, 2 facade_element2s, 9.5ms\n",
            "image 55/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5640_rotation1.jpg: 384x288 1 facade_element0, 1 facade_element2, 8.0ms\n",
            "image 56/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5640_rotation2.jpg: 384x288 1 facade_element0, 1 facade_element2, 7.5ms\n",
            "image 57/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5640_rotation3.jpg: 384x288 1 facade_element0, 1 facade_element2, 7.4ms\n",
            "image 58/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5640_rotation4.jpg: 384x288 1 facade_element0, 1 facade_element2, 7.5ms\n",
            "image 59/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5640_rotation5.jpg: 384x288 1 facade_element0, 1 facade_element2, 7.5ms\n",
            "image 60/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5640_rotation6.jpg: 384x288 1 facade_element0, 1 facade_element2, 7.7ms\n",
            "image 61/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5640_rotation7.jpg: 384x288 1 facade_element0, 1 facade_element2, 8.3ms\n",
            "image 62/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659.jpg: 288x384 1 facade_element2, 8.2ms\n",
            "image 63/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation1.jpg: 384x288 1 facade_element2, 8.3ms\n",
            "image 64/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation10.jpg: 384x288 1 facade_element2, 7.7ms\n",
            "image 65/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation11.jpg: 384x288 1 facade_element2, 7.7ms\n",
            "image 66/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation12.jpg: 384x288 1 facade_element2, 7.6ms\n",
            "image 67/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation13.jpg: 384x288 1 facade_element2, 7.5ms\n",
            "image 68/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation14.jpg: 384x288 1 facade_element2, 7.5ms\n",
            "image 69/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation15.jpg: 384x288 1 facade_element2, 7.5ms\n",
            "image 70/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation16.jpg: 384x288 1 facade_element2, 7.5ms\n",
            "image 71/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation17.jpg: 384x288 1 facade_element2, 7.5ms\n",
            "image 72/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation18.jpg: 384x288 1 facade_element2, 7.7ms\n",
            "image 73/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation19.jpg: 384x288 1 facade_element2, 7.9ms\n",
            "image 74/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation2.jpg: 384x288 1 facade_element2, 7.5ms\n",
            "image 75/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation20.jpg: 384x288 1 facade_element2, 7.7ms\n",
            "image 76/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation21.jpg: 384x288 1 facade_element2, 7.6ms\n",
            "image 77/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation22.jpg: 384x288 1 facade_element2, 7.9ms\n",
            "image 78/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation23.jpg: 384x288 1 facade_element2, 8.0ms\n",
            "image 79/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation24.jpg: 384x288 1 facade_element2, 7.6ms\n",
            "image 80/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation25.jpg: 384x288 1 facade_element2, 7.7ms\n",
            "image 81/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation26.jpg: 384x288 1 facade_element2, 7.6ms\n",
            "image 82/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation27.jpg: 384x288 1 facade_element2, 7.6ms\n",
            "image 83/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation28.jpg: 384x288 1 facade_element2, 7.5ms\n",
            "image 84/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation29.jpg: 384x288 1 facade_element2, 8.0ms\n",
            "image 85/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation3.jpg: 384x288 1 facade_element2, 7.6ms\n",
            "image 86/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation30.jpg: 384x288 1 facade_element2, 7.7ms\n",
            "image 87/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation31.jpg: 384x288 1 facade_element2, 7.5ms\n",
            "image 88/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation32.jpg: 384x288 1 facade_element2, 7.5ms\n",
            "image 89/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation33.jpg: 384x288 1 facade_element2, 7.7ms\n",
            "image 90/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation34.jpg: 384x288 1 facade_element2, 7.5ms\n",
            "image 91/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation35.jpg: 384x288 1 facade_element2, 7.5ms\n",
            "image 92/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation36.jpg: 384x288 1 facade_element2, 7.6ms\n",
            "image 93/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation37.jpg: 384x288 1 facade_element2, 7.6ms\n",
            "image 94/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation38.jpg: 384x288 1 facade_element2, 7.8ms\n",
            "image 95/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation39.jpg: 384x288 1 facade_element2, 7.6ms\n",
            "image 96/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation4.jpg: 384x288 1 facade_element2, 7.7ms\n",
            "image 97/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation5.jpg: 384x288 1 facade_element2, 10.6ms\n",
            "image 98/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation6.jpg: 384x288 1 facade_element2, 7.6ms\n",
            "image 99/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation7.jpg: 384x288 1 facade_element2, 7.4ms\n",
            "image 100/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation8.jpg: 384x288 1 facade_element2, 7.6ms\n",
            "image 101/101 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/val/IMG_5659_rotation9.jpg: 384x288 1 facade_element2, 8.3ms\n",
            "Speed: 2.1ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 288)\n",
            "Results saved to \u001b[1m/content/drive/.shortcut-targets-by-id/1eACqbAyiu4G3bgvb1aMnfl0pjOD46Nka/Neodata Hackathon 2025/YOLO/runs/yolov8n_balanced_train_pred_val5\u001b[0m\n",
            "62 labels saved to /content/drive/.shortcut-targets-by-id/1eACqbAyiu4G3bgvb1aMnfl0pjOD46Nka/Neodata Hackathon 2025/YOLO/runs/yolov8n_balanced_train_pred_val5/labels\n",
            "\n",
            "=== ONE-class Prediction vs Ground Truth (VAL) ===\n",
            "                  image  predict  pred_conf  real  gt_multiclass\n",
            "         IMG_5346 2.jpg      1.0   0.999531   1.0          False\n",
            "         IMG_5349 2.jpg      1.0   0.970374   1.0          False\n",
            "         IMG_5350 2.jpg      0.0   0.889275   1.0          False\n",
            "         IMG_5354 2.jpg      0.0   0.923114   1.0          False\n",
            "         IMG_5443 3.jpg      1.0   0.988604   1.0          False\n",
            "           IMG_5620.jpg      0.0   0.999885   0.0          False\n",
            " IMG_5620_rotation1.jpg      0.0   0.927206   0.0          False\n",
            " IMG_5620_rotation2.jpg      0.0   0.927206   0.0          False\n",
            " IMG_5620_rotation3.jpg      0.0   0.927206   0.0          False\n",
            " IMG_5620_rotation4.jpg      0.0   0.927206   0.0          False\n",
            " IMG_5620_rotation5.jpg      0.0   0.927206   0.0          False\n",
            " IMG_5620_rotation6.jpg      0.0   0.927206   0.0          False\n",
            " IMG_5620_rotation7.jpg      0.0   0.927206   0.0          False\n",
            "           IMG_5623.jpg      1.0   0.944310   NaN          False\n",
            " IMG_5623_rotation1.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation10.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation11.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation12.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation13.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation14.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation15.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation16.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation17.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation18.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation19.jpg      NaN        NaN   NaN          False\n",
            " IMG_5623_rotation2.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation20.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation21.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation22.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation23.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation24.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation25.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation26.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation27.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation28.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation29.jpg      NaN        NaN   NaN          False\n",
            " IMG_5623_rotation3.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation30.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation31.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation32.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation33.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation34.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation35.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation36.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation37.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation38.jpg      NaN        NaN   NaN          False\n",
            "IMG_5623_rotation39.jpg      NaN        NaN   NaN          False\n",
            " IMG_5623_rotation4.jpg      NaN        NaN   NaN          False\n",
            " IMG_5623_rotation5.jpg      NaN        NaN   NaN          False\n",
            " IMG_5623_rotation6.jpg      NaN        NaN   NaN          False\n",
            " IMG_5623_rotation7.jpg      NaN        NaN   NaN          False\n",
            " IMG_5623_rotation8.jpg      NaN        NaN   NaN          False\n",
            " IMG_5623_rotation9.jpg      NaN        NaN   NaN          False\n",
            "           IMG_5640.jpg      0.0   0.834490   0.0          False\n",
            " IMG_5640_rotation1.jpg      2.0   0.991042   0.0          False\n",
            " IMG_5640_rotation2.jpg      2.0   0.991042   0.0          False\n",
            " IMG_5640_rotation3.jpg      2.0   0.991042   0.0          False\n",
            " IMG_5640_rotation4.jpg      2.0   0.991042   0.0          False\n",
            " IMG_5640_rotation5.jpg      2.0   0.991042   0.0          False\n",
            " IMG_5640_rotation6.jpg      2.0   0.991042   0.0          False\n",
            " IMG_5640_rotation7.jpg      2.0   0.991042   0.0          False\n",
            "           IMG_5659.jpg      2.0   0.996234   2.0          False\n",
            " IMG_5659_rotation1.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation10.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation11.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation12.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation13.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation14.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation15.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation16.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation17.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation18.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation19.jpg      2.0   0.999951   2.0          False\n",
            " IMG_5659_rotation2.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation20.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation21.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation22.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation23.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation24.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation25.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation26.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation27.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation28.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation29.jpg      2.0   0.999951   2.0          False\n",
            " IMG_5659_rotation3.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation30.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation31.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation32.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation33.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation34.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation35.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation36.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation37.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation38.jpg      2.0   0.999951   2.0          False\n",
            "IMG_5659_rotation39.jpg      2.0   0.999951   2.0          False\n",
            " IMG_5659_rotation4.jpg      2.0   0.999951   2.0          False\n",
            " IMG_5659_rotation5.jpg      2.0   0.999951   2.0          False\n",
            " IMG_5659_rotation6.jpg      2.0   0.999951   2.0          False\n",
            " IMG_5659_rotation7.jpg      2.0   0.999951   2.0          False\n",
            " IMG_5659_rotation8.jpg      2.0   0.999951   2.0          False\n",
            " IMG_5659_rotation9.jpg      2.0   0.999951   2.0          False\n",
            "\n",
            "Saved CSV to: /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/runs/yolov8n_balanced_train/val_prediction_vs_gt_oneclass.csv\n",
            "Annotated predictions saved to: /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/runs/yolov8n_balanced_train_pred_val\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Save model</h1>"
      ],
      "metadata": {
        "id": "5jlWIbVSshjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "SRC = Path(\"/content/drive/MyDrive/Neodata Hackathon 2025/YOLO/runs/yolov8n_balanced_train/weights/best.pt\")\n",
        "DST = Path(\"/content/drive/MyDrive/Neodata Hackathon 2025/YOLO/models/facade_yolov8n_best.pt\")\n",
        "\n",
        "DST.parent.mkdir(parents=True, exist_ok=True)\n",
        "shutil.copy(SRC, DST)\n",
        "\n",
        "print(\"Saved model to:\", DST)"
      ],
      "metadata": {
        "id": "mkNp77ETgcB_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "029de6b0-a202-4f5d-c814-1ad666004410"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to: /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/models/facade_yolov8n_best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Predict for new data</h1>"
      ],
      "metadata": {
        "id": "mdDHCfesuWj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ===== PATHS =====\n",
        "MODEL_PATH = Path(\"/content/drive/MyDrive/Neodata Hackathon 2025/YOLO/models/facade_yolov8n_best.pt\")\n",
        "TEST_IMAGES = Path(\"/content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/test\")\n",
        "\n",
        "assert MODEL_PATH.exists(), f\"Missing model: {MODEL_PATH}\"\n",
        "assert TEST_IMAGES.exists(), f\"Missing test data: {TEST_IMAGES}\"\n",
        "\n",
        "# ===== LOAD MODEL =====\n",
        "model = YOLO(str(MODEL_PATH))\n",
        "\n",
        "# ===== RUN PREDICTION =====\n",
        "results = model.predict(\n",
        "    source=str(TEST_IMAGES),\n",
        "    imgsz=384,\n",
        "    conf=0.25,\n",
        "    iou=0.5,\n",
        "    save=True,        # annotated images\n",
        "    save_txt=False,   # turn off raw YOLO txt to reduce clutter\n",
        ")\n",
        "\n",
        "# ===== PROCESS RESULTS =====\n",
        "rows = []\n",
        "\n",
        "for r in results:\n",
        "    img_path = Path(r.path)\n",
        "    img_name = img_path.name\n",
        "\n",
        "    # Default: no detection\n",
        "    pred_class = None\n",
        "    pred_conf = None\n",
        "\n",
        "    if r.boxes is not None and len(r.boxes) > 0:\n",
        "        confs = r.boxes.conf.detach().cpu().numpy()\n",
        "        clss  = r.boxes.cls.detach().cpu().numpy().astype(int)\n",
        "\n",
        "        best_i = int(np.argmax(confs))\n",
        "        pred_class = int(clss[best_i])\n",
        "        pred_conf = float(confs[best_i])\n",
        "\n",
        "    rows.append({\n",
        "        \"image\": img_name,\n",
        "        \"predicted_class\": pred_class,\n",
        "        \"confidence\": None if pred_conf is None else round(pred_conf, 4),\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(rows).sort_values(\"image\")\n",
        "\n",
        "# ===== DISPLAY =====\n",
        "print(\"\\n=== MODEL PREDICTIONS (ONE CLASS PER IMAGE) ===\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "# ===== OPTIONAL: save CSV =====\n",
        "OUT_CSV = TEST_IMAGES / \"predictions_summary.csv\"\n",
        "df.to_csv(OUT_CSV, index=False)\n",
        "print(f\"\\nSaved prediction summary to: {OUT_CSV}\")\n",
        "\n",
        "print(\"\\nAnnotated images saved next to runs/ directory (Ultralytics default).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1Kv6p3gsmOJ",
        "outputId": "f37855ea-16b9-4c17-ef88-09004f10a6b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/9 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/test/0 (1).jpg: 288x384 1 facade_element0, 8.9ms\n",
            "image 2/9 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/test/0.jpg: 288x384 1 facade_element0, 7.7ms\n",
            "image 3/9 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/test/1 (1).jpg: 384x288 2 facade_element1s, 8.4ms\n",
            "image 4/9 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/test/1 (2).jpg: 288x384 2 facade_element1s, 8.3ms\n",
            "image 5/9 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/test/1 (3).jpg: 288x384 1 facade_element1, 8.5ms\n",
            "image 6/9 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/test/1 (4).jpg: 288x384 2 facade_element1s, 7.9ms\n",
            "image 7/9 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/test/1.jpg: 384x288 1 facade_element1, 8.4ms\n",
            "image 8/9 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/test/Kopija datoteke IMG_5659.jpg: 288x384 1 facade_element2, 8.4ms\n",
            "image 9/9 /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/test/null.jpg: 288x384 1 facade_element1, 7.9ms\n",
            "Speed: 2.1ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 384)\n",
            "Results saved to \u001b[1m/content/runs/detect/predict3\u001b[0m\n",
            "\n",
            "=== MODEL PREDICTIONS (ONE CLASS PER IMAGE) ===\n",
            "                       image  predicted_class  confidence\n",
            "                   0 (1).jpg                0      0.9399\n",
            "                       0.jpg                0      0.9639\n",
            "                   1 (1).jpg                1      0.9667\n",
            "                   1 (2).jpg                1      0.9689\n",
            "                   1 (3).jpg                1      0.9757\n",
            "                   1 (4).jpg                1      0.9552\n",
            "                       1.jpg                1      0.9481\n",
            "Kopija datoteke IMG_5659.jpg                2      0.8883\n",
            "                    null.jpg                1      0.9435\n",
            "\n",
            "Saved prediction summary to: /content/drive/MyDrive/Neodata Hackathon 2025/YOLO/dataset/images/test/predictions_summary.csv\n",
            "\n",
            "Annotated images saved next to runs/ directory (Ultralytics default).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EyFAJIDHtaHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C-hunQytuVSz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}